{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import os, sys\n",
    "from IPython.utils import io\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "source = \"../source\"\n",
    "sys.path.append(source)\n",
    "\n",
    "from compilation import Compiler, ScalarTracker, ActivationTracker\n",
    "import publication\n",
    "import models\n",
    "import simulate\n",
    "import two_points\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "plot_path = \"plots/2_points/comparisons/\"\n",
    "\n",
    "publication.set_color_mixed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = pd.read_csv(\"model_settings/2 points.txt\", sep=\" \", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(setting):\n",
    "    ## Load settings\n",
    "    (\n",
    "        model_type,\n",
    "        nonlinearity,\n",
    "        gain,\n",
    "        lr,\n",
    "        P,\n",
    "        L,\n",
    "        n_epochs,\n",
    "        hidden_layer,\n",
    "        dx2,\n",
    "        dy2,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "    ) = settings.loc[setting].to_numpy()\n",
    "    model_type = getattr(models, model_type)\n",
    "    if nonlinearity == \"discontinuous\":\n",
    "        nonlinearity = simulate.Discontinuous.apply\n",
    "    elif nonlinearity == \"none\":\n",
    "        nonlinearity = None\n",
    "    else:\n",
    "        nonlinearity = getattr(torch.nn.functional, nonlinearity)\n",
    "\n",
    "    threshold = 1e-4\n",
    "    n_epochs_plot = 40000\n",
    "\n",
    "    figsize = (2, 2)\n",
    "\n",
    "    ## Load data\n",
    "    def load_data(data_name):\n",
    "        data_path = f\"{plot_path}{data_name}.pkl\"\n",
    "        if os.path.exists(data_path):\n",
    "            with open(data_path, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    etas_h_dic, etas_y_dic = load_data(\"etas_h\"), load_data(\"etas_y\")\n",
    "\n",
    "    if setting in etas_h_dic.keys():\n",
    "        eta_h_opts = etas_h_dic[setting]\n",
    "        eta_y_opts = etas_y_dic[setting]\n",
    "        print(\"Fit parameters succesfully loaded from previous run.\")\n",
    "    else:\n",
    "        ## Fit effective learning rates\n",
    "        print(\"Could not find previous fit parameters, fitting...\")\n",
    "        eta_h_opts, eta_y_opts = [], []\n",
    "        for _ in range(50):\n",
    "            data, encoding = two_points.data_set(dx2, dy2, 1, 1, device)\n",
    "\n",
    "            model = model_type(\n",
    "                encoding=encoding,\n",
    "                input_size=1,\n",
    "                output_size=1,\n",
    "                hidden_dim=P,\n",
    "                n_hid_layers=L,\n",
    "                device=device,\n",
    "                init_std=gain,\n",
    "                non_linearity=nonlinearity,\n",
    "            )\n",
    "\n",
    "            ## Setup compiler\n",
    "            criterion = lambda x, y: 0.5 * nn.functional.mse_loss(x, y)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr / 2)\n",
    "            compiler = Compiler(model, criterion, optimizer)\n",
    "            compiler.trackers = {\n",
    "                \"loss\": ScalarTracker(lambda: compiler.validation([data])),\n",
    "                \"hidden\": ActivationTracker(\n",
    "                    model,\n",
    "                    lambda inputs: model(inputs)[1][hidden_layer],\n",
    "                    datasets=[data],\n",
    "                ),\n",
    "                \"output\": ActivationTracker(\n",
    "                    model, lambda inputs: model(inputs)[0], datasets=[data]\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            ## Get initial values\n",
    "            h0, y0, w0 = two_points.get_h_y_w(data, model, hidden_layer)\n",
    "\n",
    "            ## Training run\n",
    "            compiler.training_run(\n",
    "                [data], n_epochs=2 * n_epochs, batch_size=100, progress_bar=False\n",
    "            )\n",
    "\n",
    "            ## Discard trials which did not converge\n",
    "            if compiler.trackers[\"loss\"].get_entry(-1)[0][0] > 1e-2:\n",
    "                continue\n",
    "\n",
    "            ## Get data\n",
    "            data_hid = compiler.trackers[\"hidden\"].get_trace().copy()\n",
    "            data_output = compiler.trackers[\"output\"].get_trace().copy()\n",
    "            h_A = [\n",
    "                np.array(data.loc[epoch, 0, \"A\"])\n",
    "                for epoch, data in data_hid.query(\"Dataset == 0\").groupby(\"Epoch\")\n",
    "            ]\n",
    "            h_B = [\n",
    "                np.array(data.loc[epoch, 0, \"B\"])\n",
    "                for epoch, data in data_hid.query(\"Dataset == 0\").groupby(\"Epoch\")\n",
    "            ]\n",
    "            y_A = [\n",
    "                np.array(data.loc[epoch, 0, \"A\"])\n",
    "                for epoch, data in data_output.query(\"Dataset == 0\").groupby(\"Epoch\")\n",
    "            ]\n",
    "            y_B = [\n",
    "                np.array(data.loc[epoch, 0, \"B\"])\n",
    "                for epoch, data in data_output.query(\"Dataset == 0\").groupby(\"Epoch\")\n",
    "            ]\n",
    "            epochs = np.arange(0, len(h_A))\n",
    "            y_true_A, y_true_B = data[0][1].numpy(), data[1][1].numpy()\n",
    "            dy2 = np.sum((y_true_B - y_true_A) ** 2)\n",
    "            h2 = np.array([np.sum((h_A[epoch] - h_B[epoch]) ** 2) for epoch in epochs])\n",
    "            y2 = np.array([np.sum((y_A[epoch] - y_B[epoch]) ** 2) for epoch in epochs])\n",
    "            w = np.array(\n",
    "                [\n",
    "                    y2[epoch] - np.dot(y_true_A - y_true_B, y_A[epoch] - y_B[epoch])\n",
    "                    for epoch in epochs\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            ## Find etas\n",
    "            eta_h_opt, eta_y_opt, fit_loss = simulate.optimize_eta(h2, y2, w, dx2, dy2)\n",
    "            eta_h_opts.append(eta_h_opt)\n",
    "            eta_y_opts.append(eta_y_opt)\n",
    "\n",
    "        ## Save data\n",
    "        etas_h_dic[setting] = eta_h_opts\n",
    "        etas_y_dic[setting] = eta_y_opts\n",
    "        with open(f\"{plot_path}etas_h.pkl\", \"wb\") as f:\n",
    "            pickle.dump(etas_h_dic, f)\n",
    "        with open(f\"{plot_path}etas_y.pkl\", \"wb\") as f:\n",
    "            pickle.dump(etas_y_dic, f)\n",
    "\n",
    "    eta_h, eta_y = np.mean(eta_h_opts), np.mean(eta_y_opts)\n",
    "\n",
    "    print(f\"eta_h, eta_y = {eta_h}, {eta_y}\")\n",
    "\n",
    "    h0s, y0s, w0s, hs, ys, ws = [], [], [], [], [], []\n",
    "    dx2s, dy2s = [], []\n",
    "\n",
    "    N = 5\n",
    "    m = 1\n",
    "\n",
    "    print(\"Computing comparison...\")\n",
    "    for dx in np.linspace(0.5, 1.5, N):\n",
    "        for dy in np.linspace(0, 1, N):\n",
    "            dx2 = dx**2\n",
    "            dy2 = dy**2\n",
    "            variables = []\n",
    "            for _ in range(m):\n",
    "                ## Generate data\n",
    "                data, encoding = two_points.data_set(dx2, dy2, 1, 1, device)\n",
    "\n",
    "                ## Instantiate model\n",
    "                model = model_type(\n",
    "                    encoding=encoding,\n",
    "                    input_size=1,\n",
    "                    output_size=1,\n",
    "                    hidden_dim=P,\n",
    "                    n_hid_layers=L,\n",
    "                    device=device,\n",
    "                    init_std=gain,\n",
    "                    non_linearity=nonlinearity,\n",
    "                )\n",
    "\n",
    "                ## Setup compiler\n",
    "                criterion = lambda x, y: 0.5 * nn.functional.mse_loss(x, y)\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=lr / 5)\n",
    "                compiler = Compiler(model, criterion, optimizer)\n",
    "                compiler.trackers = {\n",
    "                    \"loss\": ScalarTracker(lambda: compiler.validation([data]))\n",
    "                }\n",
    "\n",
    "                ## Get initial values\n",
    "                h0, y0, w0 = two_points.get_h_y_w(data, model, hidden_layer)\n",
    "\n",
    "                ## Training run\n",
    "                compiler.training_run(\n",
    "                    [data],\n",
    "                    n_epochs=n_epochs_plot,\n",
    "                    batch_size=100,\n",
    "                    conv_thresh=threshold,\n",
    "                    progress_bar=False,\n",
    "                )\n",
    "\n",
    "                ## Discard trials which did not converge\n",
    "                if compiler.trackers[\"loss\"].get_entry(-1)[0][0] > 1e-3:\n",
    "                    break\n",
    "\n",
    "                ## Get final values\n",
    "                h, y, w = two_points.get_h_y_w(data, model, hidden_layer)\n",
    "\n",
    "                variables.append([h0, y0, w0, h, y, w])\n",
    "\n",
    "            if len(variables) > 0:\n",
    "                variables = np.mean(np.array(variables), axis=0)\n",
    "                for i, array in enumerate((h0s, y0s, w0s, hs, ys, ws)):\n",
    "                    array.append(variables[i])\n",
    "                dx2s.append(dx2)\n",
    "                dy2s.append(dy2)\n",
    "\n",
    "                clear_output(wait=True)\n",
    "\n",
    "                ## Plot\n",
    "                publication.set_color_mixed()\n",
    "                A_low = np.sqrt(eta_h / eta_y) * np.sqrt(dy2s) * np.sqrt(dx2s)\n",
    "                A_high = (\n",
    "                    np.array(h0s) / np.array(dx2s)\n",
    "                    - np.array(y0s) / np.array(h0s) * eta_h / eta_y\n",
    "                ) * np.array(dx2s)\n",
    "                h_theory = 0.5 * A_high + np.sqrt(0.25 * A_high**2 + A_low**2)\n",
    "                fig = plt.figure(figsize=figsize)\n",
    "                plt.scatter(h_theory, hs)\n",
    "                plt.xlabel(\"$||dh(\\infty)||^2$ (Theory)\")\n",
    "                plt.ylabel(\"$||dh(\\infty)||^2$ (Experiment)\")\n",
    "                plt.ylim(0, 1.1 * max(hs))\n",
    "                plt.xlim(0)\n",
    "                plt.scatter(-1, -1)\n",
    "                plt.plot(\n",
    "                    np.sort(h_theory),\n",
    "                    np.sort(h_theory),\n",
    "                    linestyle=\"--\",\n",
    "                    color=\"0.6\",\n",
    "                    zorder=0,\n",
    "                )\n",
    "                publication.plt_show(\n",
    "                    save_path=plot_path + setting + \".png\",\n",
    "                )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t-----DEFAULT-----\n",
      "Could not find previous fit parameters, fitting...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m setting \u001b[38;5;129;01min\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mindex:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetting\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mmake_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m, in \u001b[0;36mmake_plots\u001b[0;34m(setting)\u001b[0m\n\u001b[1;32m     80\u001b[0m h0, y0, w0 \u001b[38;5;241m=\u001b[39m two_points\u001b[38;5;241m.\u001b[39mget_h_y_w(data, model, hidden_layer)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m## Training run\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m## Discard trials which did not converge\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compiler\u001b[38;5;241m.\u001b[39mtrackers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_entry(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-2\u001b[39m:\n",
      "File \u001b[0;32m~/projects/universality/experiments/../source/compilation.py:233\u001b[0m, in \u001b[0;36mCompiler.training_run\u001b[0;34m(self, training_datasets, n_epochs, batch_size, progress_bar, conv_thresh)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tracker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 233\u001b[0m     \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[1;32m    236\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion, trainloader)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m/\u001b[39m n_train_data)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m trainloader, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(trainloaders, training_datasets)\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m~/projects/universality/experiments/../source/compilation.py:87\u001b[0m, in \u001b[0;36mScalarTracker.track\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39mappend(data)\n",
      "Cell \u001b[0;32mIn[3], line 68\u001b[0m, in \u001b[0;36mmake_plots.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     66\u001b[0m compiler \u001b[38;5;241m=\u001b[39m Compiler(model, criterion, optimizer)\n\u001b[1;32m     67\u001b[0m compiler\u001b[38;5;241m.\u001b[39mtrackers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: ScalarTracker(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m: ActivationTracker(\n\u001b[1;32m     70\u001b[0m         model,\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inputs: model(inputs)[\u001b[38;5;241m1\u001b[39m][hidden_layer],\n\u001b[1;32m     72\u001b[0m         datasets\u001b[38;5;241m=\u001b[39m[data],\n\u001b[1;32m     73\u001b[0m     ),\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: ActivationTracker(\n\u001b[1;32m     75\u001b[0m         model, \u001b[38;5;28;01mlambda\u001b[39;00m inputs: model(inputs)[\u001b[38;5;241m0\u001b[39m], datasets\u001b[38;5;241m=\u001b[39m[data]\n\u001b[1;32m     76\u001b[0m     ),\n\u001b[1;32m     77\u001b[0m }\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m## Get initial values\u001b[39;00m\n\u001b[1;32m     80\u001b[0m h0, y0, w0 \u001b[38;5;241m=\u001b[39m two_points\u001b[38;5;241m.\u001b[39mget_h_y_w(data, model, hidden_layer)\n",
      "File \u001b[0;32m~/projects/universality/experiments/../source/compilation.py:184\u001b[0m, in \u001b[0;36mCompiler.validation\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m    183\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 184\u001b[0m     loss_this_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     loss_this_dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    188\u001b[0m         torch\u001b[38;5;241m.\u001b[39msqueeze(loss_this_dataset)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m     loss_this_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([loss_this_dataset], [i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "etas_h, etas_y = {}, {}\n",
    "for setting in settings.index:\n",
    "    print(f\"\\t\\t\\t\\t\\t-----{setting.upper()}-----\")\n",
    "    make_plots(setting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
